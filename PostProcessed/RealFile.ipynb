{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SIZE=179\n",
    "MIN_COUNT=5\n",
    "WINDOW=6\n",
    "NEGATIVE_WORDS=6\n",
    "EPOCH_SIZE=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Miniconda2\\envs\\keras\\lib\\site-packages\\gensim\\utils.py:855: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "2017-04-10 02:31:32,259 : INFO : running C:\\ProgramData\\Miniconda2\\envs\\keras\\lib\\site-packages\\ipykernel_launcher.py -f C:\\Users\\perfe\\AppData\\Roaming\\jupyter\\runtime\\kernel-b3998bc0-27f0-4433-b0aa-ca0b922c3dd1.json\n"
     ]
    }
   ],
   "source": [
    "# gensim modules\n",
    "from gensim import utils\n",
    "from gensim.models.doc2vec import LabeledSentence\n",
    "from gensim.models import Doc2Vec\n",
    "\n",
    "# numpy\n",
    "import numpy\n",
    "\n",
    "# shuffle\n",
    "from random import shuffle\n",
    "\n",
    "# logging\n",
    "import logging\n",
    "import os.path\n",
    "import sys\n",
    "\n",
    "program = os.path.basename(sys.argv[0])\n",
    "logger = logging.getLogger(program)\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s')\n",
    "logging.root.setLevel(level=logging.INFO)\n",
    "logger.info(\"running %s\" % ' '.join(sys.argv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-04-10 02:31:32,290 : WARNING : consider setting layer size to a multiple of 4 for greater performance\n",
      "2017-04-10 02:31:32,298 : INFO : collecting all words and their counts\n",
      "2017-04-10 02:31:32,299 : INFO : PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags\n",
      "2017-04-10 02:31:32,307 : INFO : collected 2385 word types and 454 unique tags from a corpus of 454 examples and 36524 words\n",
      "2017-04-10 02:31:32,308 : INFO : Loading a fresh vocabulary\n",
      "2017-04-10 02:31:32,313 : INFO : min_count=5 retains 848 unique words (35% of original 2385, drops 1537)\n",
      "2017-04-10 02:31:32,314 : INFO : min_count=5 leaves 33541 word corpus (91% of original 36524, drops 2983)\n",
      "2017-04-10 02:31:32,318 : INFO : deleting the raw counts dictionary of 2385 items\n",
      "2017-04-10 02:31:32,320 : INFO : sample=0.0001 downsamples 552 most-common words\n",
      "2017-04-10 02:31:32,321 : INFO : downsampling leaves estimated 9895 word corpus (29.5% of prior 33541)\n",
      "2017-04-10 02:31:32,322 : INFO : estimated required memory for 848 words and 179 dimensions: 2054200 bytes\n",
      "2017-04-10 02:31:32,326 : INFO : resetting layer weights\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "class LabeledLineSentence(object):\n",
    "\n",
    "    def __init__(self, sources):\n",
    "        self.sources = sources\n",
    "\n",
    "        flipped = {}\n",
    "\n",
    "        # make sure that keys are unique\n",
    "        for key, value in sources.items():\n",
    "            if value not in flipped:\n",
    "                flipped[value] = [key]\n",
    "            else:\n",
    "                raise Exception('Non-unique prefix encountered')\n",
    "\n",
    "    def __iter__(self):\n",
    "        for source, prefix in self.sources.items():\n",
    "            with utils.smart_open(source) as fin:\n",
    "                for item_no, line in enumerate(fin):\n",
    "                    yield LabeledSentence(utils.to_unicode(line).split(), [prefix + '_%s' % item_no])\n",
    "\n",
    "    def to_array(self):\n",
    "        self.sentences = []\n",
    "        for source, prefix in self.sources.items():\n",
    "            with utils.smart_open(source) as fin:\n",
    "                for item_no, line in enumerate(fin):\n",
    "                    self.sentences.append(LabeledSentence(\n",
    "                        utils.to_unicode(line).split(), [prefix + '_%s' % item_no]))\n",
    "        return self.sentences\n",
    "\n",
    "    def sentences_perm(self):\n",
    "        shuffle(self.sentences)\n",
    "        return self.sentences\n",
    "sources = {'test-neg.txt':'TEST_NEG', 'test-pos.txt':'TEST_POS', 'train-neg.txt':'TRAIN_NEG', 'train-pos.txt':'TRAIN_POS'} #'train-unsup.txt':'TRAIN_UNS'}\n",
    "\n",
    "sentences = LabeledLineSentence(sources)\n",
    "\n",
    "\n",
    "model = Doc2Vec(min_count=MIN_COUNT, window=WINDOW, size=SIZE, sample=1e-4, negative=5, workers=7)\n",
    "\n",
    "model.build_vocab(sentences.to_array())\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-04-10 02:31:32,353 : INFO : Epoch 0\n",
      "2017-04-10 02:31:32,355 : INFO : training model with 7 workers on 848 vocabulary and 179 features, using sg=0 hs=0 sample=0.0001 negative=5 window=6\n",
      "2017-04-10 02:31:32,357 : INFO : expecting 454 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-04-10 02:31:32,467 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2017-04-10 02:31:32,469 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2017-04-10 02:31:32,484 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2017-04-10 02:31:32,491 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-04-10 02:31:32,498 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-04-10 02:31:32,500 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-04-10 02:31:32,501 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-04-10 02:31:32,502 : INFO : training on 182620 raw words (51638 effective words) took 0.1s, 371044 effective words/s\n",
      "2017-04-10 02:31:32,503 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-04-10 02:31:32,504 : INFO : Epoch 1\n",
      "2017-04-10 02:31:32,505 : INFO : training model with 7 workers on 848 vocabulary and 179 features, using sg=0 hs=0 sample=0.0001 negative=5 window=6\n",
      "2017-04-10 02:31:32,507 : INFO : expecting 454 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-04-10 02:31:32,608 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2017-04-10 02:31:32,614 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2017-04-10 02:31:32,623 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2017-04-10 02:31:32,637 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-04-10 02:31:32,639 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-04-10 02:31:32,640 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-04-10 02:31:32,641 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-04-10 02:31:32,643 : INFO : training on 182620 raw words (51626 effective words) took 0.1s, 397783 effective words/s\n",
      "2017-04-10 02:31:32,644 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-04-10 02:31:32,645 : INFO : Epoch 2\n",
      "2017-04-10 02:31:32,646 : INFO : training model with 7 workers on 848 vocabulary and 179 features, using sg=0 hs=0 sample=0.0001 negative=5 window=6\n",
      "2017-04-10 02:31:32,647 : INFO : expecting 454 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-04-10 02:31:32,746 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2017-04-10 02:31:32,749 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2017-04-10 02:31:32,768 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2017-04-10 02:31:32,781 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-04-10 02:31:32,784 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-04-10 02:31:32,785 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-04-10 02:31:32,787 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-04-10 02:31:32,788 : INFO : training on 182620 raw words (51801 effective words) took 0.1s, 383395 effective words/s\n",
      "2017-04-10 02:31:32,789 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-04-10 02:31:32,790 : INFO : Epoch 3\n",
      "2017-04-10 02:31:32,792 : INFO : training model with 7 workers on 848 vocabulary and 179 features, using sg=0 hs=0 sample=0.0001 negative=5 window=6\n",
      "2017-04-10 02:31:32,793 : INFO : expecting 454 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-04-10 02:31:32,900 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2017-04-10 02:31:32,902 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2017-04-10 02:31:32,910 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2017-04-10 02:31:32,921 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-04-10 02:31:32,923 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-04-10 02:31:32,926 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-04-10 02:31:32,927 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-04-10 02:31:32,928 : INFO : training on 182620 raw words (51552 effective words) took 0.1s, 400659 effective words/s\n",
      "2017-04-10 02:31:32,929 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCH_SIZE):\n",
    "    logger.info('Epoch %d' % epoch)\n",
    "    model.train(sentences.sentences_perm())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-04-10 02:31:32,933 : INFO : saving Doc2Vec object under ./problems.d2v, separately None\n",
      "2017-04-10 02:31:32,935 : INFO : not storing attribute syn0norm\n",
      "2017-04-10 02:31:32,935 : INFO : not storing attribute cum_table\n",
      "2017-04-10 02:31:32,955 : INFO : saved ./problems.d2v\n"
     ]
    }
   ],
   "source": [
    "model.save('./problems.d2v')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-04-10 02:31:32,984 : INFO : loading Doc2Vec object from ./problems.d2v\n",
      "2017-04-10 02:31:33,001 : INFO : loading wv recursively from ./problems.d2v.wv.* with mmap=None\n",
      "2017-04-10 02:31:33,001 : INFO : setting ignored attribute syn0norm to None\n",
      "2017-04-10 02:31:33,002 : INFO : loading docvecs recursively from ./problems.d2v.docvecs.* with mmap=None\n",
      "2017-04-10 02:31:33,003 : INFO : setting ignored attribute cum_table to None\n",
      "2017-04-10 02:31:33,005 : INFO : loaded ./problems.d2v\n"
     ]
    }
   ],
   "source": [
    "model = Doc2Vec.load('./problems.d2v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-04-10 02:31:33,015 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('binary', 0.9998446106910706),\n",
       " ('its', 0.9998146295547485),\n",
       " ('traversal', 0.9998093843460083),\n",
       " ('values', 0.999808132648468),\n",
       " ('nodes', 0.9997954964637756),\n",
       " ('null', 0.9997896552085876),\n",
       " ('wikipedia', 0.999787449836731),\n",
       " ('like', 0.9997801780700684),\n",
       " (\"node's\", 0.9997782707214355),\n",
       " ('node', 0.9997777938842773)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('tree')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  5.97662851e-02,   4.28803451e-02,   4.70422618e-02,\n",
       "        -5.40076233e-02,   6.68402063e-03,  -1.50082648e-01,\n",
       "         2.74657793e-02,   2.03467999e-02,  -1.22520123e-02,\n",
       "        -9.92507190e-02,   8.51491764e-02,  -3.82563509e-02,\n",
       "        -1.08294442e-01,  -2.27265470e-02,   5.13738915e-02,\n",
       "         1.18339937e-02,  -1.10091399e-02,   5.99853136e-02,\n",
       "         3.93416509e-02,  -6.67271987e-02,  -4.21812013e-03,\n",
       "        -3.80047373e-02,  -1.00590758e-01,  -7.55948648e-02,\n",
       "         1.05049193e-01,   9.52897221e-03,   5.21955155e-02,\n",
       "        -2.55325623e-02,   8.40167403e-02,  -1.53658139e-02,\n",
       "         4.11241502e-02,   3.58989537e-02,   7.76713490e-02,\n",
       "        -1.06333114e-01,  -1.67350844e-02,   3.32841463e-02,\n",
       "        -1.57632935e-03,   3.42021286e-02,  -2.16884781e-02,\n",
       "        -5.39727248e-02,  -6.01158552e-02,   3.98898534e-02,\n",
       "         9.34761539e-02,  -1.29023809e-02,   1.35793565e-02,\n",
       "        -3.50974873e-02,  -4.00638133e-02,  -5.38617074e-02,\n",
       "        -4.01346721e-02,  -4.77245934e-02,  -9.58492793e-03,\n",
       "         4.31412552e-03,  -3.41187455e-02,  -5.06417528e-02,\n",
       "         1.96456090e-02,  -3.47884260e-02,   2.24205945e-03,\n",
       "        -6.52015135e-02,   5.59775271e-02,  -1.10146627e-01,\n",
       "         4.00249436e-02,  -5.57690039e-02,   1.79373473e-02,\n",
       "        -2.42051408e-02,  -4.02799658e-02,   5.20241307e-03,\n",
       "         3.54185775e-02,   8.58245715e-02,   1.09513581e-01,\n",
       "        -3.82029153e-02,  -6.07817471e-02,   1.08542867e-01,\n",
       "        -9.31982771e-02,  -1.24009233e-02,  -9.63954479e-02,\n",
       "         1.14132613e-02,   1.17632933e-01,   6.22624680e-02,\n",
       "         6.05607145e-02,   4.39612232e-02,   2.07882021e-02,\n",
       "        -1.06153917e-03,   4.69293185e-02,   9.07739848e-02,\n",
       "         6.22857474e-02,   1.35518759e-01,  -5.40783294e-02,\n",
       "        -4.46769558e-02,  -3.18132490e-02,   2.18439884e-02,\n",
       "         3.48684192e-02,  -4.31917459e-02,   8.95086527e-02,\n",
       "        -7.79212788e-02,   2.41978676e-03,   8.86906907e-02,\n",
       "        -3.05284429e-02,   1.09060593e-02,  -4.08523120e-02,\n",
       "         1.16696106e-02,  -1.12748947e-02,  -4.13776413e-02,\n",
       "        -1.52923809e-02,   1.17352925e-01,   1.13387115e-01,\n",
       "        -9.82448161e-02,   2.97056953e-03,  -3.23862880e-02,\n",
       "         7.40132928e-02,   7.96997473e-02,   1.88216493e-02,\n",
       "         3.80985253e-02,   8.66194293e-02,   2.05711052e-02,\n",
       "        -6.85422346e-02,   7.12643042e-02,  -4.58103381e-02,\n",
       "         1.91826969e-02,   3.63734476e-02,  -3.29424813e-02,\n",
       "        -7.69426450e-02,  -1.95770804e-02,  -7.40276789e-03,\n",
       "         1.30397111e-01,  -3.23839486e-02,   1.41369894e-01,\n",
       "         3.34331542e-02,  -1.52717512e-02,  -8.72451738e-02,\n",
       "        -1.08942075e-03,  -2.99939793e-02,   6.66686594e-02,\n",
       "        -4.79251519e-02,   1.40319511e-01,  -1.62060112e-02,\n",
       "        -6.13630228e-02,  -1.80864520e-02,  -7.46631026e-02,\n",
       "        -1.76246334e-02,   8.77292454e-02,   8.63998212e-05,\n",
       "        -7.63171166e-02,   4.96175699e-02,   1.49884969e-02,\n",
       "         8.14340413e-02,  -1.07703216e-01,   8.08327552e-03,\n",
       "        -1.35008935e-02,  -2.28965469e-02,   5.48874065e-02,\n",
       "         8.18118900e-02,  -7.27715567e-02,  -5.47131412e-02,\n",
       "        -1.10645965e-02,   1.90187097e-02,  -4.37311679e-02,\n",
       "        -4.66155671e-02,  -4.56662178e-02,  -5.96166067e-02,\n",
       "         4.93261181e-02,   4.85473163e-02,   4.02085371e-02,\n",
       "        -9.94016416e-03,   2.75969449e-02,   4.05884832e-02,\n",
       "         4.40215729e-02,  -5.59782982e-02,   2.54078750e-02,\n",
       "        -1.76733322e-02,  -1.80947781e-02,   1.51697220e-03,\n",
       "        -7.05975294e-02,   9.98189822e-02,   1.90458968e-02,\n",
       "        -6.57768697e-02,   9.85122006e-03,   1.61924973e-01,\n",
       "         6.65216288e-03,  -3.25530916e-02], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.docvecs['TRAIN_POS_0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.06397799,  0.04854327,  0.04797371, -0.05795331,  0.00660171,\n",
       "       -0.1534498 ,  0.03194107,  0.0228539 , -0.01616059, -0.10496912,\n",
       "        0.09088878, -0.0369557 , -0.11332453, -0.02765095,  0.0504078 ,\n",
       "        0.00884504, -0.01455043,  0.06211093,  0.03845689, -0.07290225,\n",
       "       -0.00616732, -0.0376738 , -0.108957  , -0.08273569,  0.10881425,\n",
       "        0.00491898,  0.05228799, -0.03005295,  0.09054932, -0.011918  ,\n",
       "        0.03896611,  0.03833171,  0.07846707, -0.1127496 , -0.02024343,\n",
       "        0.03830462, -0.00143523,  0.03447235, -0.02882409, -0.05632888,\n",
       "       -0.06388983,  0.03593896,  0.10247084, -0.0128594 ,  0.01833743,\n",
       "       -0.03917554, -0.04268269, -0.05542992, -0.04303888, -0.04696486,\n",
       "       -0.00711204,  0.00519106, -0.03470116, -0.05322464,  0.02282947,\n",
       "       -0.03598513,  0.00314935, -0.07143622,  0.06212395, -0.1100677 ,\n",
       "        0.04408289, -0.05736115,  0.01436321, -0.03038301, -0.04009873,\n",
       "        0.00391322,  0.03688042,  0.08665289,  0.11611758, -0.04268412,\n",
       "       -0.06456831,  0.11689846, -0.09888875, -0.01691958, -0.10618167,\n",
       "        0.01231798,  0.12540117,  0.06457096,  0.06254095,  0.04444201,\n",
       "        0.02253649, -0.00178736,  0.04836582,  0.0976351 ,  0.0641029 ,\n",
       "        0.13879116, -0.05956629, -0.04766822, -0.0335811 ,  0.01716829,\n",
       "        0.03653813, -0.04898014,  0.10218182, -0.08035734,  0.00206196,\n",
       "        0.09499187, -0.03739327,  0.00655567, -0.04143206,  0.01196414,\n",
       "       -0.01052747, -0.03591378, -0.01142707,  0.13028407,  0.11882345,\n",
       "       -0.10513092,  0.00138317, -0.03331167,  0.07459576,  0.08640799,\n",
       "        0.01983847,  0.04316674,  0.08997501,  0.02059852, -0.0718722 ,\n",
       "        0.07894757, -0.04653337,  0.0235767 ,  0.03762386, -0.03026208,\n",
       "       -0.08183592, -0.01623596, -0.00898283,  0.14053169, -0.03469654,\n",
       "        0.14629695,  0.03462122, -0.02095954, -0.08613811,  0.00385015,\n",
       "       -0.02950488,  0.07048309, -0.04757645,  0.15272643, -0.01615038,\n",
       "       -0.06538066, -0.02091022, -0.07905409, -0.01128046,  0.09315216,\n",
       "       -0.00141223, -0.08067668,  0.0525566 ,  0.01733352,  0.08442032,\n",
       "       -0.11411648,  0.00193046, -0.01181014, -0.02822506,  0.05827924,\n",
       "        0.08900255, -0.07326968, -0.06142487, -0.0091971 ,  0.0209787 ,\n",
       "       -0.04607065, -0.05023961, -0.05570643, -0.06640986,  0.05073115,\n",
       "        0.04967953,  0.04539027, -0.00887198,  0.02727882,  0.04404394,\n",
       "        0.04814675, -0.06048922,  0.0240678 , -0.01590078, -0.02251719,\n",
       "       -0.00277367, -0.07413989,  0.10298481,  0.02159389, -0.06801304,\n",
       "        0.01100904,  0.17519106,  0.00426198, -0.03004618], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.docvecs['TEST_POS_0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-04-10 02:31:33,112 : INFO : Sentiment\n"
     ]
    }
   ],
   "source": [
    "logger.info('Sentiment')\n",
    "train_arrays = numpy.zeros((210+154, SIZE))\n",
    "train_labels = numpy.zeros(210+154)\n",
    "\n",
    "for i in range(210):\n",
    "    prefix_train_pos = 'TRAIN_POS_' + str(i)\n",
    "    train_arrays[i] = model.docvecs[prefix_train_pos]\n",
    "    train_labels[i] = 1\n",
    "for i in range(154):\n",
    "    prefix_train_neg = 'TRAIN_NEG_' + str(i)\n",
    "    train_arrays[210 + i] = model.docvecs[prefix_train_neg]\n",
    "    train_labels[210 + i] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_arrays = numpy.zeros((52+38, SIZE))\n",
    "test_labels = numpy.zeros(52+38)\n",
    "\n",
    "for i in range(52):\n",
    "    prefix_test_pos = 'TEST_POS_' + str(i)\n",
    "    test_arrays[i] = model.docvecs[prefix_test_pos]\n",
    "    test_labels[i] = 1\n",
    "for i in range(38):\n",
    "    prefix_test_neg = 'TEST_NEG_' + str(i)\n",
    "    test_arrays[52 + i] = model.docvecs[prefix_test_neg]\n",
    "    test_labels[52 + i] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-04-10 02:31:33,296 : INFO : Fitting\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logger.info('Fitting')\n",
    "classifier = LogisticRegression()\n",
    "classifier.fit(train_arrays, train_labels)\n",
    "\n",
    "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
    "          intercept_scaling=1, penalty='l2', random_state=None, tol=0.0001)\n",
    "\n",
    "print(classifier.score(test_arrays, test_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.577777777778\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "classifier = svm.SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
    "    decision_function_shape=None, degree=3, gamma='auto', kernel='linear',\n",
    "    max_iter=-1, probability=True, random_state=None, shrinking=True,\n",
    "    tol=0.001, verbose=False)\n",
    "classifier.fit(train_arrays, train_labels)\n",
    "print(classifier.score(test_arrays,test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "364/364 [==============================] - 0s - loss: 0.6906 - acc: 0.5659     \n",
      "Epoch 2/15\n",
      "364/364 [==============================] - 0s - loss: 0.6864 - acc: 0.5769     \n",
      "Epoch 3/15\n",
      "364/364 [==============================] - 0s - loss: 0.6858 - acc: 0.5769     \n",
      "Epoch 4/15\n",
      "364/364 [==============================] - 0s - loss: 0.6843 - acc: 0.5769     \n",
      "Epoch 5/15\n",
      "364/364 [==============================] - 0s - loss: 0.6841 - acc: 0.5769     \n",
      "Epoch 6/15\n",
      "364/364 [==============================] - 0s - loss: 0.6849 - acc: 0.5769     \n",
      "Epoch 7/15\n",
      "364/364 [==============================] - 0s - loss: 0.6851 - acc: 0.5769     \n",
      "Epoch 8/15\n",
      "364/364 [==============================] - 0s - loss: 0.6833 - acc: 0.5769     \n",
      "Epoch 9/15\n",
      "364/364 [==============================] - 0s - loss: 0.6830 - acc: 0.5769     \n",
      "Epoch 10/15\n",
      "364/364 [==============================] - 0s - loss: 0.6832 - acc: 0.5769     \n",
      "Epoch 11/15\n",
      "364/364 [==============================] - 0s - loss: 0.6841 - acc: 0.5769     \n",
      "Epoch 12/15\n",
      "364/364 [==============================] - 0s - loss: 0.6846 - acc: 0.5769     \n",
      "Epoch 13/15\n",
      "364/364 [==============================] - 0s - loss: 0.6829 - acc: 0.5769     \n",
      "Epoch 14/15\n",
      "364/364 [==============================] - 0s - loss: 0.6834 - acc: 0.5769     \n",
      "Epoch 15/15\n",
      "364/364 [==============================] - 0s - loss: 0.6865 - acc: 0.5769     \n",
      "Test loss: 0.685728515519\n",
      "Test accuracy: 0.577777777778\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['loss', 'acc']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras import optimizers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout,Input, Flatten\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding, AveragePooling1D\n",
    "from keras.models import Model\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras import backend as K\n",
    "from keras.utils import np_utils\n",
    "import numpy as np\n",
    "batch_size = 20\n",
    "num_classes = 2\n",
    "epochs = 15\n",
    "\n",
    "# input image dimensions\n",
    "img_rows, img_cols = 28, 28\n",
    "\n",
    "# # the data, shuffled and split between train and test sets\n",
    "# (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# if K.image_data_format() == 'channels_first':\n",
    "#     x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
    "#     x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
    "#     input_shape = (1, img_rows, img_cols)\n",
    "# else:\n",
    "#     x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "#     x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "#     input_shape = (img_rows, img_cols, 1)\n",
    "x_train=train_arrays.reshape(train_arrays.shape[0], SIZE,1)\n",
    "x_test=test_arrays.reshape(test_arrays.shape[0], SIZE,1)\n",
    "input_shape = (SIZE)\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "#print('x_train shape:', x_train.shape)\n",
    "#print(x_train.shape[0], 'train samples')\n",
    "#print(x_test.shape[0], 'test samples')\n",
    "y_train=np_utils.to_categorical(train_labels, 2)\n",
    "y_test=np_utils.to_categorical(test_labels, 2)\n",
    "#print(y_test)\n",
    "num_classes=2\n",
    "sequence_input = (SIZE,1)\n",
    "Kmodel = Sequential()\n",
    "Kmodel.add(Conv1D(64, kernel_size=1,activation='linear',input_shape=sequence_input))\n",
    "Kmodel.add(LeakyReLU(alpha=.001))\n",
    "#print(Kmodel.output_shape)\n",
    "Kmodel.add(AveragePooling1D(3))\n",
    "#Kmodel.add(Conv1D(128, 2, activation='linear'))\n",
    "#Kmodel.add(LeakyReLU(alpha=.001)) \n",
    "#Kmodel.add(AveragePooling1D())\n",
    "#print(Kmodel.output_shape)\n",
    "Kmodel.add(Flatten())\n",
    "#print(Kmodel.output_shape)\n",
    "Kmodel.add(Dense(128, activation='linear'))\n",
    "Kmodel.add(LeakyReLU(alpha=.001))\n",
    "#print(Kmodel.output_shape)\n",
    "Kmodel.add(Dense(num_classes, activation='softmax'))\n",
    "#print(Kmodel.output_shape)\n",
    "Kmodel.compile(optimizer='rmsprop',loss='binary_crossentropy',metrics=['accuracy'])\n",
    "Kmodel.fit(x_train, y_train,\n",
    "           batch_size=batch_size,\n",
    "           epochs=epochs,\n",
    "           verbose=1)\n",
    "score = Kmodel.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "Kmodel.metrics_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "np.random.seed(123)  # for reproducibility\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Convolution2D, MaxPooling2D, normalization\n",
    "from keras.utils import np_utils\n",
    "from keras.datasets import mnist, cifar10\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "# Basic Prepocess function for MNIST\n",
    "# Uses PCA and Normalization\n",
    "def pca_mnist(X, X_test):\n",
    "    X = X.astype('float32') / 255.\n",
    "    X_test = X_test.astype('float32') / 255.\n",
    "    X = X.reshape((len(X), np.prod(X.shape[1:])))\n",
    "    X_test = X_test.reshape((len(X_test), np.prod(X_test.shape[1:])))\n",
    "    print(X.shape)\n",
    "    pca = PCA(n_components=100)\n",
    "    pca.fit(X)\n",
    "    X = pca.inverse_transform(pca.transform(X))\n",
    "    X_test = pca.inverse_transform(pca.transform(X_test))\n",
    "    X = X.reshape(X.shape[0], 1, 28, 28)\n",
    "    X_test = X_test.reshape(X_test.shape[0], 1, 28, 28)\n",
    "    print(X.shape)\n",
    "    return X, X_test\n",
    "\n",
    "#Preprocess for cifar10\n",
    "#Doesn't really affect performance\n",
    "def pca_cifar_pre(X):\n",
    "    X2 = np.transpose(X, (3, 0, 1, 2))\n",
    "    temp1 = 0\n",
    "    temp2 = 0\n",
    "    temp3 = 0\n",
    "    for i in range(3): #individual PCA for each color \n",
    "        temp_X = X2[i][:][:][:]\n",
    "        temp_X = np.transpose(temp_X, (0, 2, 1))\n",
    "        temp_X = pca_cifar(temp_X)\n",
    "        if i == 0:\n",
    "            temp1 = temp_X\n",
    "        elif i == 1:\n",
    "            temp2 = temp_X\n",
    "        else:\n",
    "            temp3 = temp_X\n",
    "    X_temp = np.array([temp1, temp2, temp3])\n",
    "    # print(X.shape)\n",
    "    X = np.transpose(X, (1, 2, 3, 0))\n",
    "    X = X.astype('float32') / 255.\n",
    "    X = X.reshape(X.shape[0], 3, 32, 32)\n",
    "    return X\n",
    "\n",
    "\n",
    "def pca_cifar(X):\n",
    "    X = X.astype('float32') / 255.\n",
    "    X = X.reshape((len(X), np.prod(X.shape[1:])))\n",
    "    print(X.shape)\n",
    "    pca = PCA(n_components=1024)\n",
    "    pca.fit(X)\n",
    "    X = pca.inverse_transform(pca.transform(X))\n",
    "    X = X.reshape(X.shape[0], 32, 32)\n",
    "    print(X.shape)\n",
    "    return X\n",
    "\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "\n",
    "\n",
    "# 6. Preprocess class labels\n",
    "Y_train = np_utils.to_categorical(y_train, 10)\n",
    "Y_test = np_utils.to_categorical(y_test, 10)\n",
    "\n",
    "# 7. Define model architecture\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Convolution(64, kernel_size=(5, 5), data_format=\"channels_first\", activation='relu',input_shape=(1, 28, 28)))\n",
    "\n",
    "\n",
    "print(model.output_shape)\n",
    "model.add(Convolution2D(64,kernel_size=(5, 5), activation='linear')) #Convolutinal Leaky Layer\n",
    "model.add(LeakyReLU(alpha=.001))\n",
    "print(model.output_shape)\n",
    "model.add(MaxPooling2D(pool_size=(2, 2))) #Vanila Pooling\n",
    "print(model.output_shape)\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "print(model.output_shape)\n",
    "model.add(Dense(128, activation='relu'))\n",
    "print(model.output_shape)\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "\n",
    "model.summary\n",
    "\n",
    "#Compile model\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "    \n",
    "#Fit model on training data\n",
    "model.fit(X_train, Y_train, batch_size=32, epochs=20, verbose=1)\n",
    "\n",
    "\n",
    "#Evaluate model on test data\n",
    "score = model.evaluate(X_test, Y_test, verbose=1)\n",
    "print(score)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
