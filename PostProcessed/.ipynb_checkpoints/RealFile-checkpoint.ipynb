{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SIZE=30\n",
    "MIN_COUNT=5\n",
    "WINDOW=20\n",
    "NEGATIVE_WORDS=0\n",
    "EPOCH_SIZE=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Miniconda2\\envs\\keras\\lib\\site-packages\\gensim\\utils.py:855: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "2017-04-09 22:26:56,181 : INFO : running C:\\ProgramData\\Miniconda2\\envs\\keras\\lib\\site-packages\\ipykernel_launcher.py -f C:\\Users\\perfe\\AppData\\Roaming\\jupyter\\runtime\\kernel-b3998bc0-27f0-4433-b0aa-ca0b922c3dd1.json\n"
     ]
    }
   ],
   "source": [
    "# gensim modules\n",
    "from gensim import utils\n",
    "from gensim.models.doc2vec import LabeledSentence\n",
    "from gensim.models import Doc2Vec\n",
    "\n",
    "# numpy\n",
    "import numpy\n",
    "\n",
    "# shuffle\n",
    "from random import shuffle\n",
    "\n",
    "# logging\n",
    "import logging\n",
    "import os.path\n",
    "import sys\n",
    "\n",
    "program = os.path.basename(sys.argv[0])\n",
    "logger = logging.getLogger(program)\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s')\n",
    "logging.root.setLevel(level=logging.INFO)\n",
    "logger.info(\"running %s\" % ' '.join(sys.argv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-04-09 22:26:56,219 : WARNING : consider setting layer size to a multiple of 4 for greater performance\n",
      "2017-04-09 22:26:56,228 : INFO : collecting all words and their counts\n",
      "2017-04-09 22:26:56,229 : INFO : PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags\n",
      "2017-04-09 22:26:56,235 : INFO : collected 2385 word types and 454 unique tags from a corpus of 454 examples and 36524 words\n",
      "2017-04-09 22:26:56,236 : INFO : Loading a fresh vocabulary\n",
      "2017-04-09 22:26:56,240 : INFO : min_count=5 retains 848 unique words (35% of original 2385, drops 1537)\n",
      "2017-04-09 22:26:56,241 : INFO : min_count=5 leaves 33541 word corpus (91% of original 36524, drops 2983)\n",
      "2017-04-09 22:26:56,245 : INFO : deleting the raw counts dictionary of 2385 items\n",
      "2017-04-09 22:26:56,247 : INFO : sample=0.0001 downsamples 552 most-common words\n",
      "2017-04-09 22:26:56,247 : INFO : downsampling leaves estimated 9895 word corpus (29.5% of prior 33541)\n",
      "2017-04-09 22:26:56,248 : INFO : estimated required memory for 848 words and 30 dimensions: 772800 bytes\n",
      "2017-04-09 22:26:56,254 : INFO : resetting layer weights\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "class LabeledLineSentence(object):\n",
    "\n",
    "    def __init__(self, sources):\n",
    "        self.sources = sources\n",
    "\n",
    "        flipped = {}\n",
    "\n",
    "        # make sure that keys are unique\n",
    "        for key, value in sources.items():\n",
    "            if value not in flipped:\n",
    "                flipped[value] = [key]\n",
    "            else:\n",
    "                raise Exception('Non-unique prefix encountered')\n",
    "\n",
    "    def __iter__(self):\n",
    "        for source, prefix in self.sources.items():\n",
    "            with utils.smart_open(source) as fin:\n",
    "                for item_no, line in enumerate(fin):\n",
    "                    yield LabeledSentence(utils.to_unicode(line).split(), [prefix + '_%s' % item_no])\n",
    "\n",
    "    def to_array(self):\n",
    "        self.sentences = []\n",
    "        for source, prefix in self.sources.items():\n",
    "            with utils.smart_open(source) as fin:\n",
    "                for item_no, line in enumerate(fin):\n",
    "                    self.sentences.append(LabeledSentence(\n",
    "                        utils.to_unicode(line).split(), [prefix + '_%s' % item_no]))\n",
    "        return self.sentences\n",
    "\n",
    "    def sentences_perm(self):\n",
    "        shuffle(self.sentences)\n",
    "        return self.sentences\n",
    "sources = {'test-neg.txt':'TEST_NEG', 'test-pos.txt':'TEST_POS', 'train-neg.txt':'TRAIN_NEG', 'train-pos.txt':'TRAIN_POS'} #'train-unsup.txt':'TRAIN_UNS'}\n",
    "\n",
    "sentences = LabeledLineSentence(sources)\n",
    "\n",
    "\n",
    "model = Doc2Vec(min_count=MIN_COUNT, window=WINDOW, size=SIZE, sample=1e-4, negative=5, workers=7)\n",
    "\n",
    "model.build_vocab(sentences.to_array())\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-04-09 22:26:56,279 : INFO : Epoch 0\n",
      "2017-04-09 22:26:56,281 : INFO : training model with 7 workers on 848 vocabulary and 30 features, using sg=0 hs=0 sample=0.0001 negative=5 window=20\n",
      "2017-04-09 22:26:56,283 : INFO : expecting 454 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-04-09 22:26:56,387 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2017-04-09 22:26:56,391 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2017-04-09 22:26:56,407 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2017-04-09 22:26:56,420 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-04-09 22:26:56,422 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-04-09 22:26:56,425 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-04-09 22:26:56,427 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-04-09 22:26:56,427 : INFO : training on 182620 raw words (51529 effective words) took 0.1s, 377691 effective words/s\n",
      "2017-04-09 22:26:56,428 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-04-09 22:26:56,430 : INFO : Epoch 1\n",
      "2017-04-09 22:26:56,432 : INFO : training model with 7 workers on 848 vocabulary and 30 features, using sg=0 hs=0 sample=0.0001 negative=5 window=20\n",
      "2017-04-09 22:26:56,433 : INFO : expecting 454 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-04-09 22:26:56,558 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2017-04-09 22:26:56,562 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2017-04-09 22:26:56,578 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2017-04-09 22:26:56,593 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-04-09 22:26:56,596 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-04-09 22:26:56,600 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-04-09 22:26:56,602 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-04-09 22:26:56,603 : INFO : training on 182620 raw words (51763 effective words) took 0.2s, 313715 effective words/s\n",
      "2017-04-09 22:26:56,604 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-04-09 22:26:56,605 : INFO : Epoch 2\n",
      "2017-04-09 22:26:56,608 : INFO : training model with 7 workers on 848 vocabulary and 30 features, using sg=0 hs=0 sample=0.0001 negative=5 window=20\n",
      "2017-04-09 22:26:56,609 : INFO : expecting 454 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-04-09 22:26:56,734 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2017-04-09 22:26:56,740 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2017-04-09 22:26:56,748 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2017-04-09 22:26:56,761 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-04-09 22:26:56,766 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-04-09 22:26:56,768 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-04-09 22:26:56,769 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-04-09 22:26:56,770 : INFO : training on 182620 raw words (51813 effective words) took 0.2s, 334693 effective words/s\n",
      "2017-04-09 22:26:56,770 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-04-09 22:26:56,771 : INFO : Epoch 3\n",
      "2017-04-09 22:26:56,772 : INFO : training model with 7 workers on 848 vocabulary and 30 features, using sg=0 hs=0 sample=0.0001 negative=5 window=20\n",
      "2017-04-09 22:26:56,773 : INFO : expecting 454 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-04-09 22:26:56,897 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2017-04-09 22:26:56,900 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2017-04-09 22:26:56,908 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2017-04-09 22:26:56,922 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-04-09 22:26:56,926 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-04-09 22:26:56,928 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-04-09 22:26:56,929 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-04-09 22:26:56,930 : INFO : training on 182620 raw words (51835 effective words) took 0.2s, 340581 effective words/s\n",
      "2017-04-09 22:26:56,932 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-04-09 22:26:56,934 : INFO : Epoch 4\n",
      "2017-04-09 22:26:56,936 : INFO : training model with 7 workers on 848 vocabulary and 30 features, using sg=0 hs=0 sample=0.0001 negative=5 window=20\n",
      "2017-04-09 22:26:56,938 : INFO : expecting 454 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-04-09 22:26:57,061 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2017-04-09 22:26:57,063 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2017-04-09 22:26:57,081 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2017-04-09 22:26:57,092 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-04-09 22:26:57,093 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-04-09 22:26:57,095 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-04-09 22:26:57,096 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-04-09 22:26:57,097 : INFO : training on 182620 raw words (52043 effective words) took 0.2s, 339790 effective words/s\n",
      "2017-04-09 22:26:57,099 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-04-09 22:26:57,101 : INFO : Epoch 5\n",
      "2017-04-09 22:26:57,103 : INFO : training model with 7 workers on 848 vocabulary and 30 features, using sg=0 hs=0 sample=0.0001 negative=5 window=20\n",
      "2017-04-09 22:26:57,104 : INFO : expecting 454 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-04-09 22:26:57,236 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2017-04-09 22:26:57,242 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2017-04-09 22:26:57,254 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2017-04-09 22:26:57,267 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-04-09 22:26:57,269 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-04-09 22:26:57,270 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-04-09 22:26:57,271 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-04-09 22:26:57,273 : INFO : training on 182620 raw words (51868 effective words) took 0.2s, 319142 effective words/s\n",
      "2017-04-09 22:26:57,274 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-04-09 22:26:57,275 : INFO : Epoch 6\n",
      "2017-04-09 22:26:57,277 : INFO : training model with 7 workers on 848 vocabulary and 30 features, using sg=0 hs=0 sample=0.0001 negative=5 window=20\n",
      "2017-04-09 22:26:57,278 : INFO : expecting 454 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-04-09 22:26:57,411 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2017-04-09 22:26:57,420 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2017-04-09 22:26:57,427 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2017-04-09 22:26:57,445 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-04-09 22:26:57,447 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-04-09 22:26:57,448 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-04-09 22:26:57,449 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-04-09 22:26:57,450 : INFO : training on 182620 raw words (51847 effective words) took 0.2s, 316668 effective words/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-04-09 22:26:57,452 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-04-09 22:26:57,453 : INFO : Epoch 7\n",
      "2017-04-09 22:26:57,455 : INFO : training model with 7 workers on 848 vocabulary and 30 features, using sg=0 hs=0 sample=0.0001 negative=5 window=20\n",
      "2017-04-09 22:26:57,457 : INFO : expecting 454 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-04-09 22:26:57,570 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2017-04-09 22:26:57,574 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2017-04-09 22:26:57,586 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2017-04-09 22:26:57,596 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-04-09 22:26:57,600 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-04-09 22:26:57,603 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-04-09 22:26:57,604 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-04-09 22:26:57,605 : INFO : training on 182620 raw words (51647 effective words) took 0.1s, 361045 effective words/s\n",
      "2017-04-09 22:26:57,606 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-04-09 22:26:57,607 : INFO : Epoch 8\n",
      "2017-04-09 22:26:57,608 : INFO : training model with 7 workers on 848 vocabulary and 30 features, using sg=0 hs=0 sample=0.0001 negative=5 window=20\n",
      "2017-04-09 22:26:57,609 : INFO : expecting 454 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-04-09 22:26:57,738 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2017-04-09 22:26:57,742 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2017-04-09 22:26:57,753 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2017-04-09 22:26:57,765 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-04-09 22:26:57,767 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-04-09 22:26:57,768 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-04-09 22:26:57,769 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-04-09 22:26:57,770 : INFO : training on 182620 raw words (51791 effective words) took 0.2s, 330319 effective words/s\n",
      "2017-04-09 22:26:57,771 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-04-09 22:26:57,772 : INFO : Epoch 9\n",
      "2017-04-09 22:26:57,773 : INFO : training model with 7 workers on 848 vocabulary and 30 features, using sg=0 hs=0 sample=0.0001 negative=5 window=20\n",
      "2017-04-09 22:26:57,774 : INFO : expecting 454 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-04-09 22:26:57,897 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2017-04-09 22:26:57,902 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2017-04-09 22:26:57,911 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2017-04-09 22:26:57,928 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-04-09 22:26:57,930 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-04-09 22:26:57,933 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-04-09 22:26:57,935 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-04-09 22:26:57,936 : INFO : training on 182620 raw words (51900 effective words) took 0.2s, 338177 effective words/s\n",
      "2017-04-09 22:26:57,937 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCH_SIZE):\n",
    "    logger.info('Epoch %d' % epoch)\n",
    "    model.train(sentences.sentences_perm())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-04-09 22:26:57,942 : INFO : saving Doc2Vec object under ./problems.d2v, separately None\n",
      "2017-04-09 22:26:57,944 : INFO : not storing attribute syn0norm\n",
      "2017-04-09 22:26:57,945 : INFO : not storing attribute cum_table\n",
      "2017-04-09 22:26:57,956 : INFO : saved ./problems.d2v\n"
     ]
    }
   ],
   "source": [
    "model.save('./problems.d2v')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-04-09 22:26:57,969 : INFO : loading Doc2Vec object from ./problems.d2v\n",
      "2017-04-09 22:26:57,979 : INFO : loading wv recursively from ./problems.d2v.wv.* with mmap=None\n",
      "2017-04-09 22:26:57,980 : INFO : setting ignored attribute syn0norm to None\n",
      "2017-04-09 22:26:57,982 : INFO : loading docvecs recursively from ./problems.d2v.docvecs.* with mmap=None\n",
      "2017-04-09 22:26:57,983 : INFO : setting ignored attribute cum_table to None\n",
      "2017-04-09 22:26:57,984 : INFO : loaded ./problems.d2v\n"
     ]
    }
   ],
   "source": [
    "model = Doc2Vec.load('./problems.d2v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-04-09 22:26:57,996 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('binary', 0.9800577759742737),\n",
       " ('null', 0.9415110945701599),\n",
       " ('traversal', 0.9239295125007629),\n",
       " (\"nodes'\", 0.8982542157173157),\n",
       " ('depth', 0.8932375907897949),\n",
       " ('balanced', 0.8846848607063293),\n",
       " (\"node's\", 0.8796252012252808),\n",
       " ('subtrees', 0.878676176071167),\n",
       " ('root', 0.8704690933227539),\n",
       " ('level', 0.8670284152030945)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('tree')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.00251156,  0.15419485,  0.26611525, -0.35904437,  0.19920121,\n",
       "        0.24341719,  0.06151686,  0.23377679, -0.22625712, -0.36943638,\n",
       "        0.1781563 ,  0.1155684 , -0.16968213,  0.04710313,  0.52066338,\n",
       "       -0.4068217 , -0.02386617,  0.47474992,  0.28807136, -0.1757254 ,\n",
       "       -0.1343237 , -0.06848236,  0.19653124, -0.1361874 ,  0.50918865,\n",
       "       -0.14923973, -0.15550993,  0.12989785, -0.70141578, -0.30918592], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.docvecs['TRAIN_POS_0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.69357145, -0.58050078, -0.29846862,  0.39219114, -0.10331772,\n",
       "       -0.42688149,  0.18498832,  0.10081039,  0.13312499,  0.11119775,\n",
       "       -0.12664713, -0.04270788,  0.35502681, -0.22283931, -0.01384692,\n",
       "        0.08903183, -0.11225707, -0.11168535,  0.00864464,  0.09854713,\n",
       "        0.37058038,  0.09419844, -0.49387881,  0.0503021 ,  0.22622231,\n",
       "        0.0960787 , -0.16669518,  0.72537422,  0.03061169, -0.63029099], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.docvecs['TEST_POS_0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-04-09 22:26:58,079 : INFO : Sentiment\n"
     ]
    }
   ],
   "source": [
    "logger.info('Sentiment')\n",
    "train_arrays = numpy.zeros((210+154, SIZE))\n",
    "train_labels = numpy.zeros(210+154)\n",
    "\n",
    "for i in range(210):\n",
    "    prefix_train_pos = 'TRAIN_POS_' + str(i)\n",
    "    train_arrays[i] = model.docvecs[prefix_train_pos]\n",
    "    train_labels[i] = 1\n",
    "for i in range(154):\n",
    "    prefix_train_neg = 'TRAIN_NEG_' + str(i)\n",
    "    train_arrays[210 + i] = model.docvecs[prefix_train_neg]\n",
    "    train_labels[210 + i] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_arrays = numpy.zeros((52+38, SIZE))\n",
    "test_labels = numpy.zeros(52+38)\n",
    "\n",
    "for i in range(52):\n",
    "    prefix_test_pos = 'TEST_POS_' + str(i)\n",
    "    test_arrays[i] = model.docvecs[prefix_test_pos]\n",
    "    test_labels[i] = 1\n",
    "for i in range(38):\n",
    "    prefix_test_neg = 'TEST_NEG_' + str(i)\n",
    "    test_arrays[52 + i] = model.docvecs[prefix_test_neg]\n",
    "    test_labels[52 + i] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-12-6a672e603f58>, line 10)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-12-6a672e603f58>\"\u001b[1;36m, line \u001b[1;32m10\u001b[0m\n\u001b[1;33m    print classifier.score(test_arrays, test_labels)\u001b[0m\n\u001b[1;37m                   ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logger.info('Fitting')\n",
    "classifier = LogisticRegression()\n",
    "classifier.fit(train_arrays, train_labels)\n",
    "\n",
    "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
    "          intercept_scaling=1, penalty='l2', random_state=None, tol=0.0001)\n",
    "\n",
    "print classifier.score(test_arrays, test_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "classifier = svm.SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
    "    decision_function_shape=None, degree=3, gamma='auto', kernel='linear',\n",
    "    max_iter=-1, probability=True, random_state=None, shrinking=True,\n",
    "    tol=0.001, verbose=False)\n",
    "classifier.fit(train_arrays, train_labels)\n",
    "print classifier.score(train_arrays,train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_arrays[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class_probabilities = classifier.predict_proba(test_arrays)\n",
    "#print(class_probabilities)\n",
    "for n,i in enumerate(class_probabilities):\n",
    "    print(i*100,n)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Y_train=np_utils.to_categorical(train_labels, 2)\n",
    "Y_test=np_utils.to_categorical(test_labels, 2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (364, 30)\n",
      "364 train samples\n",
      "90 test samples\n",
      "(None, 26, 128)\n",
      "(None, 22, 128)\n",
      "(None, 2816)\n",
      "(None, 128)\n",
      "(None, 2)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking model input: expected conv1d_30_input to have 3 dimensions, but got array with shape (364, 30)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m--------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-54-dd2accb62713>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     61\u001b[0m           \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m           \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m           verbose=1)\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Test loss:'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda2\\envs\\keras\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[0;32m    843\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    844\u001b[0m                               \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 845\u001b[1;33m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m    846\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    847\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda2\\envs\\keras\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[0;32m   1403\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1404\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1405\u001b[1;33m             batch_size=batch_size)\n\u001b[0m\u001b[0;32m   1406\u001b[0m         \u001b[1;31m# prepare validation data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1407\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda2\\envs\\keras\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, check_batch_axis, batch_size)\u001b[0m\n\u001b[0;32m   1293\u001b[0m                                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_feed_input_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1294\u001b[0m                                     \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1295\u001b[1;33m                                     exception_prefix='model input')\n\u001b[0m\u001b[0;32m   1296\u001b[0m         y = _standardize_input_data(y, self._feed_output_names,\n\u001b[0;32m   1297\u001b[0m                                     \u001b[0moutput_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda2\\envs\\keras\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    119\u001b[0m                                  \u001b[1;34m' to have '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshapes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m                                  \u001b[1;34m' dimensions, but got array with shape '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 121\u001b[1;33m                                  str(array.shape))\n\u001b[0m\u001b[0;32m    122\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mref_dim\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshapes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking model input: expected conv1d_30_input to have 3 dimensions, but got array with shape (364, 30)"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout,Input, Flatten\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras.utils import np_utils\n",
    "import numpy as np\n",
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs = 12\n",
    "\n",
    "# input image dimensions\n",
    "img_rows, img_cols = 28, 28\n",
    "\n",
    "# # the data, shuffled and split between train and test sets\n",
    "# (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# if K.image_data_format() == 'channels_first':\n",
    "#     x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
    "#     x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
    "#     input_shape = (1, img_rows, img_cols)\n",
    "# else:\n",
    "#     x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "#     x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "#     input_shape = (img_rows, img_cols, 1)\n",
    "x_train=train_arrays\n",
    "x_test=test_arrays\n",
    "input_shape = (SIZE)\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "y_train=np_utils.to_categorical(train_labels, 2)\n",
    "y_test=np_utils.to_categorical(test_labels, 2)\n",
    "\n",
    "num_classes=2\n",
    "sequence_input = (30,1)\n",
    "model = Sequential()\n",
    "model.add(Conv1D(128, kernel_size=5,activation='relu',input_shape=sequence_input))\n",
    "print(model.output_shape)\n",
    "#model.add(MaxPooling1D(5))\n",
    "model.add(Conv1D(128, 5, activation='relu'))\n",
    "print(model.output_shape)\n",
    "model.add(Flatten())\n",
    "print(model.output_shape)\n",
    "model.add(Dense(128, activation='relu'))\n",
    "print(model.output_shape)\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "print(model.output_shape)\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1)\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "# x = Conv1D(128, 5, activation='relu')(x_train)\n",
    "# x = MaxPooling1D(5)(x)\n",
    "# x = Conv1D(128, 5, activation='relu')(x)\n",
    "# x = MaxPooling1D(5)(x)\n",
    "# x = Conv1D(128, 5, activation='relu')(x)\n",
    "# x = MaxPooling1D(35)(x)\n",
    "# x = Flatten()(x)\n",
    "# x = Dense(128, activation='relu')(x)\n",
    "# preds = Dense(len(labels_index), activation='softmax')(x)\n",
    "\n",
    "# modelKeras = Model(sequence_input, preds)\n",
    "# modelKeras.compile(loss='categorical_crossentropy',\n",
    "#               optimizer='rmsprop',\n",
    "#               metrics=['acc'])\n",
    "\n",
    "# modelKeras.fit(x_train, y_train,\n",
    "#           batch_size=128,\n",
    "#           epochs=10)\n",
    "\n",
    "# score = modelKeras.evaluate(x_test, y_test, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(123)  # for reproducibility\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Convolution2D, MaxPooling2D, normalization\n",
    "from keras.utils import np_utils\n",
    "from keras.datasets import mnist, cifar10\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "# Basic Prepocess function for MNIST\n",
    "# Uses PCA and Normalization\n",
    "def pca_mnist(X, X_test):\n",
    "    X = X.astype('float32') / 255.\n",
    "    X_test = X_test.astype('float32') / 255.\n",
    "    X = X.reshape((len(X), np.prod(X.shape[1:])))\n",
    "    X_test = X_test.reshape((len(X_test), np.prod(X_test.shape[1:])))\n",
    "    print(X.shape)\n",
    "    pca = PCA(n_components=100)\n",
    "    pca.fit(X)\n",
    "    X = pca.inverse_transform(pca.transform(X))\n",
    "    X_test = pca.inverse_transform(pca.transform(X_test))\n",
    "    X = X.reshape(X.shape[0], 1, 28, 28)\n",
    "    X_test = X_test.reshape(X_test.shape[0], 1, 28, 28)\n",
    "    print(X.shape)\n",
    "    return X, X_test\n",
    "\n",
    "#Preprocess for cifar10\n",
    "#Doesn't really affect performance\n",
    "def pca_cifar_pre(X):\n",
    "    X2 = np.transpose(X, (3, 0, 1, 2))\n",
    "    temp1 = 0\n",
    "    temp2 = 0\n",
    "    temp3 = 0\n",
    "    for i in range(3): #individual PCA for each color \n",
    "        temp_X = X2[i][:][:][:]\n",
    "        temp_X = np.transpose(temp_X, (0, 2, 1))\n",
    "        temp_X = pca_cifar(temp_X)\n",
    "        if i == 0:\n",
    "            temp1 = temp_X\n",
    "        elif i == 1:\n",
    "            temp2 = temp_X\n",
    "        else:\n",
    "            temp3 = temp_X\n",
    "    X_temp = np.array([temp1, temp2, temp3])\n",
    "    # print(X.shape)\n",
    "    X = np.transpose(X, (1, 2, 3, 0))\n",
    "    X = X.astype('float32') / 255.\n",
    "    X = X.reshape(X.shape[0], 3, 32, 32)\n",
    "    return X\n",
    "\n",
    "\n",
    "def pca_cifar(X):\n",
    "    X = X.astype('float32') / 255.\n",
    "    X = X.reshape((len(X), np.prod(X.shape[1:])))\n",
    "    print(X.shape)\n",
    "    pca = PCA(n_components=1024)\n",
    "    pca.fit(X)\n",
    "    X = pca.inverse_transform(pca.transform(X))\n",
    "    X = X.reshape(X.shape[0], 32, 32)\n",
    "    print(X.shape)\n",
    "    return X\n",
    "\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "\n",
    "\n",
    "# 6. Preprocess class labels\n",
    "Y_train = np_utils.to_categorical(y_train, 10)\n",
    "Y_test = np_utils.to_categorical(y_test, 10)\n",
    "\n",
    "# 7. Define model architecture\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Convolution(64, kernel_size=(5, 5), data_format=\"channels_first\", activation='relu',input_shape=(1, 28, 28)))\n",
    "\n",
    "\n",
    "print(model.output_shape)\n",
    "model.add(Convolution2D(64,kernel_size=(5, 5), activation='linear')) #Convolutinal Leaky Layer\n",
    "model.add(LeakyReLU(alpha=.001))\n",
    "print(model.output_shape)\n",
    "model.add(MaxPooling2D(pool_size=(2, 2))) #Vanila Pooling\n",
    "print(model.output_shape)\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "print(model.output_shape)\n",
    "model.add(Dense(128, activation='relu'))\n",
    "print(model.output_shape)\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "\n",
    "model.summary\n",
    "\n",
    "#Compile model\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "    \n",
    "#Fit model on training data\n",
    "model.fit(X_train, Y_train, batch_size=32, epochs=20, verbose=1)\n",
    "\n",
    "\n",
    "#Evaluate model on test data\n",
    "score = model.evaluate(X_test, Y_test, verbose=1)\n",
    "print(score)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
